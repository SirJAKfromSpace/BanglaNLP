{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(R_CNN, self).__init__()\n",
    "\n",
    "        in_nc = 3\n",
    "        nf = 64\n",
    "        hdn = 300\n",
    "        nclass = 23 #dekhabet class\n",
    "        \n",
    "        self.convs = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_nc, nf, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.MaxPool2d(2, 2), #64 filters, 32*64\n",
    "            \n",
    "            nn.Conv2d(nf, nf*2, 3, 1, 1), \n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.MaxPool2d(2, 2), #128 filters, 16*32\n",
    "            \n",
    "            nn.Conv2d(nf*2, nf*4, 3, 1, 1), \n",
    "            nn.BatchNorm2d(nf*4),\n",
    "            \n",
    "            nn.Conv2d(nf*4, nf*4, 3, 1, 1), \n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.MaxPool2d(2,2), #256 filters, 40*16\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(nf*4, nf*4, 3, 1, 1), \n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(nf*4, nf*8, 3, 1, 1), \n",
    "            nn.BatchNorm2d(nf*8),\n",
    "            \n",
    "            nn.Conv2d(nf*8, nf*8, 3, 1, 1), \n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(nf*8, nf*8, 3, 1, 1), \n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.MaxPool2d((2, 1)),  \n",
    "            \n",
    "            nn.Conv2d(nf*8, nf*8, 2, 1, 0), \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.bilstm = nn.Sequential(\n",
    "                        BidirectionalLSTM(nf*8, hdn, hdn),\n",
    "                        BidirectionalLSTM(hdn, hdn, nclass),\n",
    "                    )\n",
    "        \n",
    "        self.lgsftMx = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.convs(x)\n",
    "        out = out.squeeze(2)\n",
    "        out = out.permute(2, 0, 1) #ctc expects [width,batch,label]\n",
    "        \n",
    "        \n",
    "        out = self.bilstm(out)\n",
    "        out = F.log_softmax(out, dim=2)\n",
    "        \n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate Model And Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = R_CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CTCLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(5, 3, 128, 256)\n",
    "input = input.to(device)\n",
    "\n",
    "target =  [[5,1,5,3,0,2,1,7,20,11],\n",
    "           [5,1,5,3,0,2,1,7,20,11],\n",
    "           [5,1,5,3,0,2,1,7,20,11], \n",
    "           [5,1,5,3,0,2,1,7,20,11],\n",
    "           [5,1,5,3,0,2,1,7,20,11]]\n",
    "\n",
    "\n",
    "\n",
    "target = torch.FloatTensor(target)\n",
    "target = target.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  1.,  5.,  3.,  0.,  2.,  1.,  7., 20., 11.],\n",
      "        [ 5.,  1.,  5.,  3.,  0.,  2.,  1.,  7., 20., 11.],\n",
      "        [ 5.,  1.,  5.,  3.,  0.,  2.,  1.,  7., 20., 11.],\n",
      "        [ 5.,  1.,  5.,  3.,  0.,  2.,  1.,  7., 20., 11.],\n",
      "        [ 5.,  1.,  5.,  3.,  0.,  2.,  1.,  7., 20., 11.]], device='cuda:0')\n",
      "tensor([7, 3, 6, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "T = 15      # Input sequence length\n",
    "C = 22      # Number of classes (including blank)\n",
    "N = 5      # Batch size\n",
    "S = 9      # Target sequence length of longest target in batch\n",
    "S_min = 2  # Minimum target length, for demonstration purposes\n",
    "\n",
    "# target = torch.randint(low=1, high=23, size=(N, S), dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,))\n",
    "                               \n",
    "# input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "\n",
    "print(target)\n",
    "print(target_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 5, 23])\n",
      "tensor(10.3430, device='cuda:0', grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "pred = model(input)\n",
    "\n",
    "print(pred.shape)\n",
    "\n",
    "preds_size = Variable(torch.LongTensor([pred.size(0)] * 5))\n",
    "\n",
    "cost = criterion(pred, target, preds_size, target_lengths)\n",
    "\n",
    "print(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(trainloader_pixel)\n",
    "ctc_loss_list = []\n",
    "acc_list = []\n",
    "batch_size= 25\n",
    "num_epochs = 2500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    trainiter = iter(trainiter)\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        spectros, lbls, lbl_lens = trainIter_pixel.next()\n",
    "        \n",
    "        spectros = spectos.to(device)\n",
    "        lbls = lbls.to(device)\n",
    "        lbl_lens.to(device)\n",
    "        \n",
    "        pred = model(spectros)\n",
    "        preds_size = Variable(torch.LongTensor([pred.size(0)] * batch_size))\n",
    "        \n",
    "\n",
    "        cost = criterion(pred, lbls, preds_size, lbl_lens)/batch_size\n",
    "        \n",
    "        #backprop and optimize!\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "      \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch No [{}/{}]  {:.4f}'.format(epoch+1,num_epochs,d_loss.item()))\n",
    "        ctc_loss_list.append(d_loss.item())\n",
    "\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print('Epoch No {}  reached saving model'.format(epoch+1)\n",
    "        torch.save(model.state_dict(), 'outputModel/KDNet_epoch_{}.pkl'.format(epoch+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
